{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple-Auto-Encoder-MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-acme/CV/blob/master/Simple_Auto_Encoder_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4lAfi30JUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ZRHiBA8zQi",
        "colab_type": "code",
        "outputId": "1343a99e-e3a0-4154-c08a-cfe2d993fb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3972
        }
      },
      "source": [
        "#Let's also create a separate encoder model:\n",
        "\n",
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "#As well as the decoder model:\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "#Now let's train our autoencoder to reconstruct MNIST digits.\n",
        "\n",
        "#First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
        "\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "\n",
        "#Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "#We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)\n",
        "\n",
        "#Now let's train our autoencoder for 50 epochs:\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "#After 50 epochs, the autoencoder seems to reach a stable train/test loss value of about 0.11. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib.\n",
        "\n",
        "# encode and decode some digits\n",
        "# note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "# use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.4262 - val_loss: 0.4205\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.4150 - val_loss: 0.4099\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.4049 - val_loss: 0.4004\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.3959 - val_loss: 0.3918\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.3877 - val_loss: 0.3841\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.3804 - val_loss: 0.3771\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.3737 - val_loss: 0.3707\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3677 - val_loss: 0.3649\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3621 - val_loss: 0.3597\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3571 - val_loss: 0.3548\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.3524 - val_loss: 0.3504\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.3481 - val_loss: 0.3463\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3442 - val_loss: 0.3425\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3406 - val_loss: 0.3390\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.3372 - val_loss: 0.3357\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.3341 - val_loss: 0.3327\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3312 - val_loss: 0.3299\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3285 - val_loss: 0.3273\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3259 - val_loss: 0.3249\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3236 - val_loss: 0.3226\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3214 - val_loss: 0.3204\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3193 - val_loss: 0.3184\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3173 - val_loss: 0.3165\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3155 - val_loss: 0.3147\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3138 - val_loss: 0.3131\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3121 - val_loss: 0.3115\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3106 - val_loss: 0.3100\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3091 - val_loss: 0.3085\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3077 - val_loss: 0.3072\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3064 - val_loss: 0.3059\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.3052 - val_loss: 0.3047\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.3040 - val_loss: 0.3035\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3029 - val_loss: 0.3024\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3018 - val_loss: 0.3014\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.3008 - val_loss: 0.3004\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2998 - val_loss: 0.2994\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2989 - val_loss: 0.2985\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2980 - val_loss: 0.2976\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2971 - val_loss: 0.2968\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2963 - val_loss: 0.2960\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2955 - val_loss: 0.2952\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2948 - val_loss: 0.2945\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2941 - val_loss: 0.2938\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2934 - val_loss: 0.2931\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.2927 - val_loss: 0.2925\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2921 - val_loss: 0.2918\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2915 - val_loss: 0.2912\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2909 - val_loss: 0.2906\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2903 - val_loss: 0.2901\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2898 - val_loss: 0.2895\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2892 - val_loss: 0.2890\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.2887 - val_loss: 0.2885\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.2882 - val_loss: 0.2880\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2877 - val_loss: 0.2876\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2873 - val_loss: 0.2871\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2868 - val_loss: 0.2867\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2864 - val_loss: 0.2862\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2860 - val_loss: 0.2858\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2856 - val_loss: 0.2854\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2852 - val_loss: 0.2850\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2848 - val_loss: 0.2846\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.2844 - val_loss: 0.2843\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.2841 - val_loss: 0.2839\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.2837 - val_loss: 0.2836\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.2834 - val_loss: 0.2832\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2831 - val_loss: 0.2829\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2828 - val_loss: 0.2826\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.2825 - val_loss: 0.2823\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.2822 - val_loss: 0.2820\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2819 - val_loss: 0.2817\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.2816 - val_loss: 0.2814\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.2813 - val_loss: 0.2812\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2810 - val_loss: 0.2809\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2808 - val_loss: 0.2806\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2805 - val_loss: 0.2804\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2803 - val_loss: 0.2801\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.2800 - val_loss: 0.2799\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.2798 - val_loss: 0.2796\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.2796 - val_loss: 0.2794\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2793 - val_loss: 0.2792\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2791 - val_loss: 0.2790\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.2789 - val_loss: 0.2788\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2787 - val_loss: 0.2785\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2785 - val_loss: 0.2783\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2783 - val_loss: 0.2781\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2781 - val_loss: 0.2780\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2779 - val_loss: 0.2778\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2777 - val_loss: 0.2776\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2775 - val_loss: 0.2774\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2774 - val_loss: 0.2772\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2772 - val_loss: 0.2770\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2770 - val_loss: 0.2769\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2768 - val_loss: 0.2767\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2767 - val_loss: 0.2765\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2765 - val_loss: 0.2764\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2764 - val_loss: 0.2762\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2762 - val_loss: 0.2761\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2761 - val_loss: 0.2759\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2759 - val_loss: 0.2758\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.2758 - val_loss: 0.2756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmUXVWZN+C3SAJkgEBCQqSZgyAy\nyqyNNihLlElQUBratkEQW2xxYLCVVgTUtUBRRATp1SggIsqgIEirNKCItAuaoZkbkECYQ0LISAKp\n749vsdn7ULdSSereOufW8/z1Hvapc3f85VRVtnvo6e3tDQAAAACG3kpD3QEAAAAA/j8DNQAAAAA1\nYaAGAAAAoCYM1AAAAADUhIEaAAAAgJowUAMAAABQEyP7a+zp6XF299CZ0dvbO2kwHiTHodPb29sz\nGM+R4ZDyLnYB72JX8C52Ae9iV/AudgHvYlfwLnaBVu+iGTX1NW2oOwBEhHcR6sK7CPXgXYR68C52\nMQM1AAAAADVhoAYAAACgJgzUAAAAANSEgRoAAACAmjBQAwAAAFATBmoAAAAAasJADQAAAEBNjBzq\nDjB8HHvssakePXp00bb11lun+sADD2z5jHPOOSfVf/7zn4u2iy66aEW7CAAAAEPKjBoAAACAmjBQ\nAwAAAFATBmoAAAAAasIeNbTVpZdemur+9p7JLVmypGXbUUcdleo99tijaLvppptS/fjjjw+0iwyh\nTTfdtLh+4IEHUn3MMcek+qyzzupYn4a7sWPHpvr0009Pdf7uRUTcfvvtqT7ooIOKtmnTprWpdwAA\nnbfmmmumev311x/Q11R/H/rc5z6X6nvuuSfVDz30UHHfXXfdtTxdpMuYUQMAAABQEwZqAAAAAGrC\n0icGVb7UKWLgy53yJS//+Z//meqNN964uG/fffdN9dSpU4u2Qw89NNXf/OY3B/S5DK23ve1txXW+\n7G369Omd7g4R8aY3vSnVRx55ZKqrSxK33377VO+zzz5F29lnn92m3vGa7bbbLtVXXHFF0bbhhhu2\n7XPf+973Ftf3339/qp944om2fS4Dk/+MjIi46qqrUv3pT3861eeee25x36uvvtrejnWZyZMnp/rn\nP/95qm+55ZbivvPOOy/Vjz32WNv79Zrx48cX1+9617tSfd1116V68eLFHesTNMHee++d6v32269o\n22233VK9ySabDOh51SVNG2ywQapXWWWVll83YsSIAT2f7mZGDQAAAEBNGKgBAAAAqAlLn1hhO+yw\nQ6oPOOCAlvfde++9qa5OJ5wxY0aq586dm+qVV165uO/WW29N9TbbbFO0TZw4cYA9pi623Xbb4nre\nvHmpvvLKKzvdnWFp0qRJxfUFF1wwRD1hWey5556p7m/69GCrLq05/PDDU33wwQd3rB+8Lv/Z94Mf\n/KDlfd///vdTff755xdtCxYsGPyOdZH8tJeI8veZfJnRs88+W9w3VMud8lP5Isrv8/my1Ycffrj9\nHWug1VdfvbjOl9NvueWWqa6ePmopWX3l2yUcffTRqc6XeEdEjB49OtU9PT0r/LnV001hWZhRAwAA\nAFATBmoAAAAAasJADQAAAEBNdHSPmupRzfm6wKeeeqpoW7hwYaovvvjiVD/zzDPFfdbXDr38ON/q\nes58HXe+p8LTTz89oGd/4QtfKK7f+ta3trz3mmuuGdAzGVr5+u78uNiIiIsuuqjT3RmWPvOZz6R6\n//33L9p22mmnZX5efvRrRMRKK73+/wHcddddqf7DH/6wzM/mdSNHvv4je6+99hqSPlT3vvj85z+f\n6rFjxxZt+Z5TtE/+/q277rot77vkkktSnf+ORd/WWmutVF966aVF24QJE1Kd7wv0L//yL+3vWAsn\nnnhiqjfaaKOi7aijjkq135v7duihh6b661//etG23nrr9fk11b1sXnjhhcHvGIMi/954zDHHtPWz\nHnjggVTn/w5icOVHpOffryPKPVPzY9UjIpYsWZLqc889N9V/+tOfivvq8L3SjBoAAACAmjBQAwAA\nAFATHV36dNpppxXXG2644YC+Lp+yOWfOnKKtk1PKpk+fnurqn+W2227rWD/q5uqrr051Pg0tosxr\n5syZy/zs6nGvo0aNWuZnUC9vectbUl1dKlGdXk57fOc730l1PgV0eX3wgx9seT1t2rRUf+QjHynu\nqy6joX+77757qt/+9renuvrzqJ2qxxTny1HHjBlTtFn61B7V49i//OUvD+jr8qWlvb29g9qnbrTd\ndtulujp1PnfyySd3oDdvtMUWWxTX+VLxK6+8smjzs7Vv+XKY7373u6nOj7yPaP2+nHXWWcV1vpx7\neX7nZemqS1zyZUz50pXrrruuuO/ll19O9ezZs1Nd/TmV/17629/+tmi75557Uv3f//3fqb7jjjuK\n+xYsWNDy+SybfLuEiPIdy3/XrP69GKidd9451a+88krR9uCDD6b65ptvLtryv3eLFi1ars8eCDNq\nAAAAAGrCQA0AAABATRioAQAAAKiJju5Rkx/HHRGx9dZbp/r+++8v2jbffPNU97dOeJdddkn1E088\nkepWR+n1JV+T9vzzz6c6P3a66vHHHy+uh/MeNbl8P4rlddxxx6V60003bXlfvj60r2vq6fjjj091\n9e+L96h9rr322lTnx2cvr/wY0rlz5xZtG2ywQarzY2L/8pe/FPeNGDFihfvRzaprs/PjlR955JFU\nf+Mb3+hYnz7wgQ907LPo21ZbbVVcb7/99i3vzX+/+c1vftO2PnWDyZMnF9cf+tCHWt778Y9/PNX5\n743tlu9L8/vf/77lfdU9aqr7O/L/HXvssanOj1wfqOq+a+973/tSXT3iO9/Ppp17WnSj/vaN2Wab\nbVKdH8lcdeutt6Y6/3flY489Vty3/vrrpzrfmzRicPb0o2/5mMDRRx+d6uo7tvrqq/f59U8++WRx\n/cc//jHVf/3rX4u2/N8h+V6JO+20U3Ff/j1hr732KtruuuuuVOdHfA82M2oAAAAAasJADQAAAEBN\ndHTp0/XXX9/vda56rNprqkeDbrvttqnOpy/tuOOOA+7XwoULU/3QQw+lurocK58ClU87Z8Xts88+\nqc6Pulx55ZWL+5577rlU/+u//mvRNn/+/Db1jhWx4YYbFtc77LBDqvP3LcIxhoPp7/7u74rrzTbb\nLNX59N2BTuWtTu3Mpx/nR11GRLz73e9OdX9HB//zP/9zqs8555wB9WM4OfHEE4vrfPp3PsW+uvRs\nsOU/+6p/r0wF77z+luRUVZcJ0Nq3v/3t4vof/uEfUp3/fhkR8Ytf/KIjfap65zvfmeq11167aPvx\nj3+c6p/85Ced6lKj5MtyIyIOO+ywPu+7++67i+tnn3021XvssUfL548fPz7V+bKqiIiLL7441c88\n88zSOzuMVX/3/+lPf5rqfKlTRLn0t7/lgLnqcqdcdWsL2uOHP/xhcZ0vW+vvqO187OB///d/U/2l\nL32puC//t33VO97xjlTnv4eef/75xX35GEP+PSAi4uyzz0715ZdfnurBXgprRg0AAABATRioAQAA\nAKiJji59GgyzZs0qrm+44YY+7+tvWVV/8inF1WVW+RSrSy+9dLmeT9/y5TDVKY+5/H/3m266qa19\nYnBUl0rkOnlaxnCQLzP72c9+VrT1N5U0l5/ElU/n/NrXvlbc199Sw/wZn/jEJ1I9adKk4r7TTjst\n1auuumrR9v3vfz/VixcvXlq3u8aBBx6Y6uopAw8//HCqO3lCWr58rbrU6cYbb0z1iy++2KkuDWvv\nete7WrZVT5Ppb+khpd7e3uI6/7v+1FNPFW3tPLVn9OjRxXU+pf9Tn/pUqqv9Pfzww9vWp26RL2WI\niFhttdVSnZ8SU/29Jf/59Pd///epri63mDp1aqqnTJlStP3qV79K9fvf//5Uz5w5c0B973bjxo1L\ndXVrg3x7hBkzZhRt3/rWt1JtC4R6qf5el5+2dMQRRxRtPT09qc7/bVBdFn/66aenenm3S5g4cWKq\n89NHTzrppOK+fBuW6rLJTjGjBgAAAKAmDNQAAAAA1ISBGgAAAICaaNweNe0wefLkVP/gBz9I9Uor\nleNY+bHR1pSumF/+8pfF9Xvf+94+77vwwguL6+pxtdTfVltt1bIt36OEFTdy5Ovf0ge6J011r6eD\nDz441dW14AOV71HzzW9+M9VnnHFGcd+YMWNSXf27cNVVV6X6kUceWa5+NNFBBx2U6vx/n4jy51O7\n5fsdHXrooal+9dVXi/tOPfXUVA+nvYQ6LT9ONK+rqmv277zzzrb1aTjZe++9i+v82PN8b6bqfgoD\nle+JsttuuxVtu+yyS59fc9llly3XZw1nq6yySnGd7/Pzne98p+XX5Uf9/uhHP0p1/v06ImLjjTdu\n+Yx8/5R27nHUVPvvv3+qv/jFLxZt+ZHZ+RH1ERGzZ89ub8dYbtXvZccdd1yq8z1pIiKefPLJVOf7\nxf7lL39Zrs/O955Zb731irb835bXXnttqqt70+aq/b3oootS3c79+cyoAQAAAKgJAzUAAAAANWHp\nU0QcffTRqc6Pj60eBf7ggw92rE/d6E1velOqq1O38+mo+XKLfFp9RMTcuXPb1DsGUz5V+7DDDiva\n7rjjjlT/7ne/61ifeF1+tHP1SNflXe7USr6EKV9CExGx4447DupnNdH48eOL61bLHCKWf1nF8siP\nVc+X0d1///3FfTfccEPH+jScDfRd6eTfkW5z5plnFte77757qtdZZ52iLT8iPZ8Sv99++y3XZ+fP\nqB67nXv00UdTXT0amqXLj9auype3VZfnt7LDDjsM+LNvvfXWVPtd9o36W9KZ/944ffr0TnSHQZAv\nP4p449Lp3CuvvJLqnXfeOdUHHnhgcd9b3vKWPr9+wYIFxfXmm2/eZx1R/p679tprt+xT7tlnny2u\nO7Xs24waAAAAgJowUAMAAABQE8Ny6dPf/u3fFtfV3cVfk+9AHhFxzz33tK1Pw8Hll1+e6okTJ7a8\n7yc/+Umqh9NpL91kjz32SPWECROKtuuuuy7V+UkKDK7qqXW5fFppu+VT+qt96q+PJ510Uqo/+tGP\nDnq/6qJ6Csnf/M3fpPqSSy7pdHeSqVOn9vnf/RwcGv0tsRiMU4eIuP3224vrrbfeOtXbbrtt0fa+\n970v1flJJs8//3xx3wUXXDCgz85PELnrrrta3nfLLbek2u9Hy676PTVfqpYvL6wur8hPrzzggANS\nXT0lJn8Xq21HHnlkqvO877vvvgH1vdtVl7jk8vftq1/9atH2q1/9KtVOuauX//qv/yqu86XS+b8T\nIiLWX3/9VH/ve99LdX9LQfOlVNVlVv1ptdxpyZIlxfWVV16Z6s985jNF29NPPz3gz1sRZtQAAAAA\n1ISBGgAAAICaMFADAAAAUBPDco+avfbaq7geNWpUqq+//vpU//nPf+5Yn7pVvv53u+22a3nfjTfe\nmOrq+lOaZ5tttkl1dX3pZZdd1unuDBuf/OQnU11daztU9t1331S/7W1vK9ryPlb7m+9R083mzJlT\nXOdr7PM9MiLK/Z5mzpw5qP2YPHlycd1qv4Cbb755UD+X1nbddddUH3LIIS3vmz17dqodXTt4Zs2a\nlerqMfT59QknnLDCn7XxxhunOt/XK6L8nnDssceu8GcNZ7///e+L6/zdyfehqe4b02qfjOrzjj76\n6FT/+te/Ltre/OY3pzrf7yL/uT2cTZo0KdXV3wfyvdy+8pWvFG0nnnhiqs8999xU58ehR5R7oDz8\n8MOpvvfee1v2aYsttiiu838X+l67dNUjs/P9ndZYY42iLd8vNt9L9oUXXijue/zxx1Od/73I/90R\nEbHTTjstc3/PO++84vpLX/pSqvP9pzrJjBoAAACAmjBQAwAAAFATw2bp0+jRo1OdH/MWEbFo0aJU\n58tuFi9e3P6OdZnqsdv5tLF8iVlVPrV37ty5g98x2m7KlCmpfuc735nqBx98sLgvP+6OwZUvM+qk\nfMpyRMRb3/rWVOffA/pTPdZ2uHz/rU4Nzo/c/dCHPlS0XXPNNak+44wzlvmzttxyy+I6X26x4YYb\nFm2tpvrXZUndcJD/PO3vKPvf/e53negObZQv56i+e/nSqur3SZZNdcnohz/84VTny7LHjx/f8hln\nnXVWqqvL3hYuXJjqK664omjLl3bsueeeqZ46dWpx33A9dv1b3/pWqj//+c8P+Ovy742f+tSn+qwH\nS/7+5Vs2HHzwwYP+Wd2uupQofz+Wx4UXXlhc97f0KV9ynv9d+/GPf1zclx//PVTMqAEAAACoCQM1\nAAAAADVhoAYAAACgJobNHjXHHXdcqqtHxF533XWpvuWWWzrWp270hS98objecccd+7zvl7/8ZXHt\nSO7m+6d/+qdU50f9/uY3vxmC3tBJX/7yl4vr/IjS/jz22GOp/tjHPla05UcwDif598LqMb177713\nqi+55JJlfvaMGTOK63wvjLXWWmtAz6iu4aZ9Wh2RXl3b/8Mf/rAT3WEQHXTQQcX1P/7jP6Y63z8h\n4o3H0zJ48uO18/ftkEMOKe7L37l8P6F8T5qqU045pbjefPPNU73ffvv1+byIN/4sHC7yPUouvfTS\nou2nP/1pqkeOLP/put5666W6v728BkO+H1/+9yU/Ijwi4tRTT21rP/j/jj/++FQvyz5Bn/zkJ1O9\nPL9LdZIZNQAAAAA1YaAGAAAAoCa6dulTPkU8IuLf/u3fUv3SSy8VbSeffHJH+jQcDPRIvU9/+tPF\ntSO5m2+DDTbo87/PmjWrwz2hE6699tpUb7bZZsv1jPvuuy/VN9988wr3qRs88MADqc6Pjo2I2Hbb\nbVO9ySabLPOz8+Nnqy644ILi+tBDD+3zvupx4gyeddddt7iuLr94zfTp04vr2267rW19oj3e//73\nt2z79a9/XVz/z//8T7u7Q5TLoPJ6eVW/V+bLefKlT7vvvntx34QJE1JdPU68m+VHIVe/p2266aYt\nv+4973lPqkeNGpXqk046qbiv1VYMyytfmrz99tsP6rNp7Ygjjkh1vuSsuiQud++99xbXV1xxxeB3\nrE3MqAEAAACoCQM1AAAAADXRVUufJk6cmOrvfe97RduIESNSnU/Zj4i49dZb29sx3iCf2hkRsXjx\n4mV+xuzZs1s+I5/+OH78+JbPWGONNYrrgS7dyqdonnDCCUXb/PnzB/SMbrPPPvv0+d+vvvrqDvdk\n+Mqn4vZ3+kF/0+7PO++8VK+zzjot78ufv2TJkoF2sbDvvvsu19cNV3feeWef9WB49NFHB3Tflltu\nWVzfc889g9qP4ewd73hHcd3qHa6emkjzVL8Hz5s3L9Xf/va3O90dOuDnP/95qvOlTx/5yEeK+/Kt\nAWzNsHTXX399n/89XyocUS59euWVV1L9ox/9qLjv3//931P92c9+tmhrtRyV9tlpp52K6/z747hx\n41p+Xb6lRn7KU0TEyy+/PEi9az8zagAAAABqwkANAAAAQE0YqAEAAACoicbvUZPvPXPdddeleqON\nNirue+SRR1KdH9XN0Lj77rtX+Bm/+MUviuunn3461WuvvXaqq+t/B9szzzxTXH/9619v6+fVxa67\n7lpcT5kyZYh6wmvOOeecVJ922mkt78uPf+1vf5mB7j0z0PvOPffcAd1H5+X7G/V1/Rp70rRPvs9e\n1YwZM1J95plndqI7DLJ8n4T8d5SIiOeeey7VjuPuTvnPyfzn8wc+8IHivq9+9aup/tnPfla0PfTQ\nQ23qXff57W9/W1znv5vnRzkfeeSRxX2bbLJJqnfbbbcBfdb06dOXo4cMRHUvw9VWW63P+/J9viLK\nfaD+9Kc/DX7HOsSMGgAAAICaMFADAAAAUBONX/o0derUVG+//fYt78uPXc6XQTG4qkefV6d0DqaD\nDjpoub4uP5avvyUbV111Vapvu+22lvf98Y9/XK5+NN0BBxxQXOfLEO+4445U/+EPf+hYn4a7K664\nItXHHXdc0TZp0qS2fe7zzz9fXN9///2p/sQnPpHqfHki9dLb29vvNe235557tmx7/PHHUz179uxO\ndIdBli99qr5f11xzTcuvy6f6r7nmmqnO/07QLHfeeWeqv/KVrxRtp59+eqq/8Y1vFG0f/ehHU71g\nwYI29a475L+HRJTHo3/4wx9u+XW77757y7ZXX3011fk7+8UvfnF5ukgL+fe8448/fkBfc/HFFxfX\nN95442B2aciYUQMAAABQEwZqAAAAAGrCQA0AAABATTRuj5oNNtiguK4ev/aa6v4M+XG0tM8HP/jB\n4jpfWzhq1KgBPWOLLbZI9bIcrX3++een+rHHHmt53+WXX57qBx54YMDPJ2LMmDGp3muvvVred9ll\nl6U6X9NLe02bNi3VBx98cNG2//77p/qYY44Z1M+tHkl/9tlnD+rzab9VV121ZZu9ENon/7mY77lX\ntXDhwlQvXry4rX2i8/Kfk4ceemjR9rnPfS7V9957b6o/9rGPtb9jtN2FF15YXB911FGprv5OffLJ\nJ6f67rvvbm/HGq76c+uzn/1sqseNG5fqHXbYobhv8uTJqa7+W+Kiiy5K9UknnTQIveQ1eSb33Xdf\nqvv7t2P+DuT5dhMzagAAAABqwkANAAAAQE00bulTftRrRMT666/f53033XRTce2o0aFx2mmnrdDX\nH3LIIYPUEwZDPuV+1qxZRVt+nPmZZ57ZsT7Rt+qx6Pl1vmS0+j113333TXWe6XnnnVfc19PTk+p8\nmirNdNhhhxXXL774YqpPOeWUTndn2FiyZEmqb7vttqJtyy23TPXDDz/csT7ReUcccUSqP/7xjxdt\n//Ef/5Fq72L3ef7554vrPfbYI9XVpTcnnHBCqqtL5Ojfs88+m+r895z8yPOIiF122SXVX/va14q2\n5557rk29493vfneq11133VT39+/3fFlovjy4m5hRAwAAAFATBmoAAAAAaqKnvylFPT09tVgvtOuu\nu6b62muvLdryXaJzO+20U3FdnVLcALf39vbusPTblq4uOQ5Hvb29PUu/a+lkOKS8i13Au9i/q6++\nurg+44wzUn3DDTd0ujutdPW7uM466xTXp556aqpvv/32VDf9VLXh+i7mv8vmp/dElEtTzznnnKIt\nX2a8aNGiNvVumXX1u1gX1ZNt3/72t6d65513TvXyLj8eru9il+mKd/Guu+5K9VZbbdXyvtNPPz3V\n+VLApmv1LppRAwAAAFATBmoAAAAAasJADQAAAEBNNOJ47ne+852pbrUnTUTEI488kuq5c+e2tU8A\n0C3y40oZGk899VRxffjhhw9RT2iHm2++OdX5UbTQyoEHHlhc5/t4bLLJJqle3j1qoC4mTJiQ6p6e\n17drqR6J/t3vfrdjfaoDM2oAAAAAasJADQAAAEBNNGLpU3/yaYDvec97Uj1z5syh6A4AAMAKeeml\nl4rrjTbaaIh6Au11xhln9FmfcsopxX1PP/10x/pUB2bUAAAAANSEgRoAAACAmjBQAwAAAFATPb29\nva0be3paN9Jut/f29u4wGA+S49Dp7e3tWfpdSyfDIeVd7ALexa7gXewC3sWu4F3sAt7FruBd7AKt\n3kUzagAAAABqwkANAAAAQE0s7XjuGRExrRMd4Q02GMRnyXFoyLA7yLH5ZNgd5Nh8MuwOcmw+GXYH\nOTZfywz73aMGAAAAgM6x9AkAAACgJgzUAAAAANSEgRoAAACAmjBQAwAAAFATBmoAAAAAasJADQAA\nAEBNGKgBAAAAqAkDNQAAAAA1YaAGAAAAoCYM1AAAAADUhIEaAAAAgJowUAMAAABQEwZqAAAAAGrC\nQA0AAABATRioAQAAAKgJAzUAAAAANWGgBgAAAKAmDNQAAAAA1ISBGgAAAICaMFADAAAAUBMGagAA\nAABqwkANAAAAQE2M7K+xp6ent1Md4Q1m9Pb2ThqMB8lx6PT29vYMxnNkOKS8i13Au9gVvItdwLvY\nFbyLXcC72BW8i12g1btoRk19TRvqDgAR4V2EuvAuQj14F6EevItdzEANAAAAQE0YqAEAAACoCQM1\nAAAAADVhoAYAAACgJgzUAAAAANREv8dz11FPT+uT5PK2gd4XEdHb29tnXTXQ+1g6OTafDLuDHJtP\nht1Bjs0nw+4gx+aTYXcY7jmaUQMAAABQEwZqAAAAAGqiNkufqtOSWk1nGjFiRHHfSiut1Gdbf/dV\nLVmyJNWvvvpqql955ZXivrwt/5qIekyPqgM5Np8Mu4Mcm0+G3UGOzSfD7iDH5pNhd5DjwJhRAwAA\nAFATBmoAAAAAasJADQAAAEBNdHSPmv7Wo1XXlo0c+XrXVllllVSPGTOmuG+11VZL9fjx41M9duzY\n4r78GdW1ZPPmzUv1Sy+91GcdETFnzpxUL1iwoGhbtGhRqvN1bNU1bd1Ajs0nw+4gx+aTYXeQY/PJ\nsDvIsflk2B3kuOLMqAEAAACoCQM1AAAAADXR9qVP+TSn6lFZo0aNSvWqq65atOVTmyZMmJDqtdde\nu7hvvfXW67OePHlycV8+dSo/bisiYtasWamePn16qqdNm1bc98QTT6T62WefLdry6VILFy5MdX9H\nfTWJHJufowybn2GEHLshRxk2P8MIOXZDjjJsfoYRcuyGHGXY/Awj5DjYOZpRAwAAAFATBmoAAAAA\naqItS59aTXvKpzxFlNOS1lhjjaItn8K04YYbpnqjjTYq7nvzm9/cZ9uUKVNaflZ1WtILL7yQ6nza\nUz71KqKcplWdzpXvKN3f7s/5dXUX6rqRY/NzlGHzM4yQYzfkKMPmZxghx27IUYbNzzBCjt2Qowyb\nn2GEHNuZoxk1AAAAADVhoAYAAACgJgzUAAAAANRE2/eoGTFiRKpXWWWV4r78KK7qsVr5urPNNtus\nzzoiYuONN071Ouusk+rx48cX9+Xr5Kpr1UaPHt1nfxcvXlzcN3/+/FTPmTOnaMuP6VqwYEHLZ+TH\ndDVpzaEcm5mjDJufYYQcuyFHGTY/wwg5dkOOMmx+hhFy7IYcZdj8DCPk2M4czagBAAAAqAkDNQAA\nAAA1MShLn/IpTxHlEVb9TYEaN25cqtdaa62ibd111011fkxX/t8jyqlO+fSi/OitiHK6UbW/eVs+\nVSqfohURsfrqq6d67NixRVu4RsAVAAAKDklEQVT+Z6se4dUUcmx+jjJsfoYRcoxofo4ybH6GEXKM\naH6OMmx+hhFyjGh+jjJsfoYRcozoXI7N/BsCAAAA0IUM1AAAAADUhIEaAAAAgJpoy/HcuXzd1siR\n5cflx2NVj9WaOHFiqtdcc82Wz3jxxRdTnR+dlR+pVTVmzJjiurom7TVLlixp+Yz+5F9XfUbdj1hr\nRY7Nz1GGzc8wQo7dkKMMm59hhBy7IUcZNj/DCDl2Q44ybH6GEXIc7BzNqAEAAACoCQM1AAAAADXR\n9qVPuerxVfl0pnw6VEQ5TSn/updeeqm4b+bMmal+7rnnUv3yyy8X9+XPrx4J1qpP1WlU+fWCBQuK\ntvzz8uPCmjp1rT9ybD4Zdgc5Np8Mu4Mcm0+G3UGOzSfD7iDHFWdGDQAAAEBNGKgBAAAAqIm2LH1q\nNe2np6enuF555ZVTXZ0Cteqqq6Y6n1KU7/YcEfHMM8/02VbdJXrcuHF9fm71+pVXXkl1dQpUPv1q\n3rx5RVv+dfmfv/q/RZOmtsmx+TnKsPkZRsixG3KUYfMzjJBjN+Qow+ZnGCHHbshRhs3PMEKO7czR\njBoAAACAmjBQAwAAAFATBmoAAAAAamJQ9qgZ6Pqr6jFdq6yySqqra9Xy9WNLlixJ9cKFC4v78jVi\nY8eOTXX1KK4pU6akevLkyUVb/tkzZsxIdXWt2pw5cwbUj6aSY/NzlGHzM4yQY7UfTSTD5mcYIcdq\nP5pIhs3PMEKO1X40kQybn2GEHKv9aCczagAAAABqwkANAAAAQE205XjuXH4016hRo4q2/CiufDpU\nRDldKp8CVbX66qunOp/2tO666xb35VOg8iO7IiJefvnlVM+cOTPVixcvLu7Lr/Ojw6ryvlePJmsq\nOTY/Rxk2P8MIOXZDjjJsfoYRcuyGHGXY/Awj5NgNOcqw+RlGyHGwczSjBgAAAKAmDNQAAAAA1ETb\nlz6NGDHi9Q8bWX5cvsNzdWfofNpTPt2o+oxJkyaleurUqamuToGaMGFCqqs7NefTnvLPqk5zyqcz\nVfuRX+f3dctUNjk2P0cZNj/DCDl2Q44ybH6GEXLshhxl2PwMI+TYDTnKsPkZRsjR0icAAACALmWg\nBgAAAKAmDNQAAAAA1ERH96jJ16ZFlOu4quvHFixYkOp8HVv1qK/8mK7x48eneuzYsS37NG/evOJ6\nzpw5fbZV16rl69Gq/cj/nP2tT8vbent7W95XN3IsNTFHGZaamGGEHKuamKMMS03MMEKOVU3MUYal\nJmYYIceqJuYow1ITM4yQY9WK5mhGDQAAAEBNGKgBAAAAqIm2LH3Kp/nkU4OqR3HlU4zmz59ftM2e\nPTvV+fSo6tSml19+uc+vqU63yj87n/IUEfHCCy+kOp8ClR8VFlH+Wary6UxNmqLWHzk2P0cZNj/D\nCDl2Q44ybH6GEXLshhxl2PwMI+TYDTnKsPkZRsixnTmaUQMAAABQEwZqAAAAAGrCQA0AAABATbRl\nj5p8XdhA16pVj86aNWtWqvP1aIsWLWr5ufkauep9+dq1/HkR5dq1alsr1fVo+Z8lX+PW5PWHcmx+\njjJsfoYRcuyGHGXY/Awj5NgNOcqw+RlGyLEbcpRh8zOMkGM7czSjBgAAAKAmDNQAAAAA1MSgLH3K\npx5FDHwKVG7x4sXFdT4VadSoUaleuHBhcd/cuXNTnU9lGj16dHFffrxXdVpSq2PFqvfl06qqU6zy\no8Ty6VBNIsfm5yjD5mcYIceI5ucow+ZnGCHHiObnKMPmZxghx4jm5yjD5mcYIceIzuVoRg0AAABA\nTRioAQAAAKiJji59Gjmy/Lh8atOqq65atOVTlsaNG5fq6tSm1VZbrc/78joiYsyYManOpytFlNOq\n8ulLCxYsKO7Ld6ieP39+0ZZPiWq1E3TdybH5Ocqw+RlGyDGi+TnKsPkZRsgxovk5yrD5GUbIMaL5\nOcqw+RlGyDGiczmaUQMAAABQEwZqAAAAAGrCQA0AAABATQzKHjVV+fFW/a3VWnnllVOdrzmLiJg4\ncWKqJ0yYkOrVV1+9uC9vy79mjTXWaPm5+XFeEeWRYC+++GKqn3/++eK+F154oeUzWq13qx71Vb2u\nMzk2P0cZNj/DCDl2Q44ybH6GEXLshhxl2PwMI+TYDTnKsPkZRsixnTmaUQMAAABQEwZqAAAAAGpi\nUJY+Vaf15NOe8uOr8mlCEeVxWflxXhHlMVv51KYpU6YU97Wa9pQfFRZRTlnKpzlFREyfPj3Vjz76\naKoff/zx4r5nnnkm1S+99FLRNtApUHUmx+bnKMPmZxghx4jm5yjD5mcYIceI5ucow+ZnGCHHiObn\nKMPmZxghx4jO5WhGDQAAAEBNGKgBAAAAqAkDNQAAAAA10ZbjufO1WgsWLEh1dY1YfgxWftxWRMSk\nSZNSna996+npafm58+bNS3V1XdyTTz6Z6v/7v/8r2u69995UP/jgg6meNm1acd/MmTP7/KyIct1d\n3t8mrTmskmPzc5Rh8zOMkGM35CjD5mcYIcduyFGGzc8wQo7dkKMMm59hhBzbmaMZNQAAAAA1YaAG\nAAAAoCbacjx3PgUqn4qUTyFa2jPyKUX5M6rHY40fP77PZ8yaNau474knnkj1I488UrT99a9/TXU+\nVar6jPnz56d68eLFRVs3TF+TY/NzlGHzM4yQY0Tzc5Rh8zOMkGNE83OUYfMzjJBjRPNzlGHzM4yQ\nY0TncjSjBgAAAKAmDNQAAAAA1ERPf9N1enp6VnguT75b84gRI4q2lVdeOdXjxo0r2tZcc81Ur7XW\nWn3+94iI0aNHpzqfhpRPV4oopzNVpzbl06ryXZ0XLVpU3Ndqh+e+rgfB7b29vTsMxoPkOHQ59vb2\ntt6ufBnI0LuYPSPVchw472LzMwzvYkQ0P0fvYvMzDO9iRDQ/R+9i8zMM72JEND/HVu+iGTUAAAAA\nNWGgBgAAAKAmDNQAAAAA1ETb96ipPK+4Xmml18eJquvYRo0alep8TdvIkeWJ4vnX5X+W/KiwiHKd\nWV5X7x3oerQOHKlWqzWHlecV13JsrU7rfyvPK65l2C/vYjQ/R+9i8zMM72JEND9H72LzMwzvYkQ0\nP0fvYvMzDO9iRDQ/R3vUAAAAANScgRoAAACAmhi59FsGT3XaUH9TivJpSflxWdVpVNXr5fms/Lq/\nqU0dmL7WCHJsPhl2Bzk2nwy7gxybT4bdQY7NJ8PuIMcVZ0YNAAAAQE0YqAEAAACoCQM1AAAAADXR\n0T1qqga6Rqx65Bb1Isfmk2F3kGPzybA7yLH5ZNgd5Nh8MuwOclx2ZtQAAAAA1ISBGgAAAICaWNrS\npxkRMa0THeENNhjEZ8lxaMiwO8ix+WTYHeTYfDLsDnJsPhl2Bzk2X8sMe+pwRjgAAAAAlj4BAAAA\n1IaBGgAAAICaMFADAAAAUBMGagAAAABqwkANAAAAQE38P6qbK+KIUuRkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOoMROFr-_Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}